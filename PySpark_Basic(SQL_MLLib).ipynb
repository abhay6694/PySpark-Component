{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- x: integer (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- workclass: string (nullable = true)\n",
      " |-- fnlwgt: integer (nullable = true)\n",
      " |-- education: string (nullable = true)\n",
      " |-- educational-num: integer (nullable = true)\n",
      " |-- marital-status: string (nullable = true)\n",
      " |-- occupation: string (nullable = true)\n",
      " |-- relationship: string (nullable = true)\n",
      " |-- race: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- capital-gain: integer (nullable = true)\n",
      " |-- capital-loss: integer (nullable = true)\n",
      " |-- hours-per-week: integer (nullable = true)\n",
      " |-- native-country: string (nullable = true)\n",
      " |-- income: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "url = \"https://raw.githubusercontent.com/guru99-edu/R-Programming/master/adult_data.csv\"\n",
    "from pyspark import SparkFiles\n",
    "sc.addFile(url)\n",
    "\n",
    "df = sqlContext.read.csv(SparkFiles.get(\"adult_data.csv\"), header=True, inferSchema= True)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|   education|count|\n",
      "+------------+-----+\n",
      "|   Preschool|   83|\n",
      "|     1st-4th|  247|\n",
      "|     5th-6th|  509|\n",
      "|   Doctorate|  594|\n",
      "|        12th|  657|\n",
      "|         9th|  756|\n",
      "| Prof-school|  834|\n",
      "|     7th-8th|  955|\n",
      "|        10th| 1389|\n",
      "|  Assoc-acdm| 1601|\n",
      "|        11th| 1812|\n",
      "|   Assoc-voc| 2061|\n",
      "|     Masters| 2657|\n",
      "|   Bachelors| 8025|\n",
      "|Some-college|10878|\n",
      "|     HS-grad|15784|\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby('education').count().sort('count',ascending=True).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20211"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.filter(df.age>40).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------+\n",
      "|      marital-status|sum(capital-gain)|\n",
      "+--------------------+-----------------+\n",
      "|           Separated|           890219|\n",
      "|       Never-married|          6195095|\n",
      "|Married-spouse-ab...|           395015|\n",
      "|            Divorced|          5264450|\n",
      "|             Widowed|           916332|\n",
      "|   Married-AF-spouse|           109950|\n",
      "|  Married-civ-spouse|         38932760|\n",
      "+--------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby('marital-status').agg({'capital-gain': 'sum'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"age_square\", col(\"age\")**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------------+\n",
      "|      native-country|count(native-country)|\n",
      "+--------------------+---------------------+\n",
      "|  Holand-Netherlands|                    1|\n",
      "|             Hungary|                   19|\n",
      "|            Honduras|                   20|\n",
      "|            Scotland|                   21|\n",
      "|Outlying-US(Guam-...|                   23|\n",
      "|          Yugoslavia|                   23|\n",
      "|                Laos|                   23|\n",
      "|     Trinadad&Tobago|                   27|\n",
      "|            Cambodia|                   28|\n",
      "|                Hong|                   30|\n",
      "|            Thailand|                   30|\n",
      "|             Ireland|                   37|\n",
      "|              France|                   38|\n",
      "|             Ecuador|                   45|\n",
      "|                Peru|                   46|\n",
      "|              Greece|                   49|\n",
      "|           Nicaragua|                   49|\n",
      "|                Iran|                   59|\n",
      "|              Taiwan|                   65|\n",
      "|            Portugal|                   67|\n",
      "+--------------------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby('native-country').agg({'native-country': 'count'}).sort(asc(\"count(native-country)\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.filter(df['native-country'] !='Holand-Netherlands')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------+\n",
      "|workclass_encoded|workclass_vec|\n",
      "+-----------------+-------------+\n",
      "|              0.0|(9,[0],[1.0])|\n",
      "|              0.0|(9,[0],[1.0])|\n",
      "|              2.0|(9,[2],[1.0])|\n",
      "|              0.0|(9,[0],[1.0])|\n",
      "|              3.0|(9,[3],[1.0])|\n",
      "|              0.0|(9,[0],[1.0])|\n",
      "|              3.0|(9,[3],[1.0])|\n",
      "|              1.0|(9,[1],[1.0])|\n",
      "|              0.0|(9,[0],[1.0])|\n",
      "|              0.0|(9,[0],[1.0])|\n",
      "+-----------------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "\n",
    "stringIndexer = StringIndexer(inputCol=\"workclass\", outputCol=\"workclass_encoded\")\n",
    "model = stringIndexer.fit(df)\n",
    "indexed = model.transform(df)\n",
    "encoder = OneHotEncoder(dropLast=False, inputCol=\"workclass_encoded\", outputCol=\"workclass_vec\")\n",
    "encoded = encoder.transform(indexed)\n",
    "encoded[['workclass_encoded','workclass_vec']].show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator\n",
    "CATE_FEATURES = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'gender', 'native-country']\n",
    "stages = [] # stages in our Pipeline\n",
    "for categoricalCol in CATE_FEATURES:\n",
    "    stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol + \"Index\")\n",
    "    encoder = OneHotEncoderEstimator(inputCols=[stringIndexer.getOutputCol()],\n",
    "                                     outputCols=[categoricalCol + \"classVec\"])\n",
    "    stages += [stringIndexer, encoder]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert label into label indices using the StringIndexer\n",
    "# label_stringIdx =  StringIndexer(inputCol=\"label\", outputCol=\"newlabel\")\n",
    "# stages += [label_stringIdx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of continuous features\n",
    "CONTI_FEATURES  = ['age', 'fnlwgt','capital-gain', 'capital-loss', 'hours-per-week']\n",
    "assemblerInputs = [c + \"classVec\" for c in CATE_FEATURES] + CONTI_FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "stages += [assembler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Pipeline.\n",
    "pipeline = Pipeline(stages=stages)\n",
    "pipelineModel = pipeline.fit(df)\n",
    "model = pipelineModel.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import DenseVector\n",
    "input_data = model.rdd.map(lambda x: (x[\"age\"], DenseVector(x[\"features\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = sqlContext.createDataFrame(input_data, [\"label\", \"features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test sets\n",
    "train_data, test_data = df_train.randomSplit([.8,.2],seed=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import `LinearRegression`\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Initialize `lr`\n",
    "lr = LogisticRegression(labelCol=\"label\",\n",
    "                        featuresCol=\"features\",\n",
    "                        maxIter=10,\n",
    "                        regParam=0.3)\n",
    "\n",
    "# Fit the data to the model\n",
    "linearModel = lr.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: DenseMatrix([[-3.36257343e-04, -1.19392432e-04, -1.13536880e-04, ...,\n",
      "              -2.19076373e-09, -5.70804746e-08, -2.70520816e-05],\n",
      "             [-3.36257343e-04, -1.19392432e-04, -1.13536880e-04, ...,\n",
      "              -2.19076373e-09, -5.70804746e-08, -2.70520816e-05],\n",
      "             [-3.36257343e-04, -1.19392432e-04, -1.13536880e-04, ...,\n",
      "              -2.19076373e-09, -5.70804746e-08, -2.70520816e-05],\n",
      "             ...,\n",
      "             [-1.53513259e-03,  5.03380085e-03, -6.88393521e-04, ...,\n",
      "              -3.61400848e-10, -3.46254471e-07, -5.21171278e-05],\n",
      "             [-5.14673589e-04, -3.59901036e-04, -3.41789670e-04, ...,\n",
      "              -6.60308109e-09, -1.71983183e-07, -4.08102124e-05],\n",
      "             [-1.57819275e-03, -1.27281197e-03,  3.49861650e-03, ...,\n",
      "               1.01872719e-07,  3.87923655e-06, -1.99779135e-04]])\n",
      "Intercept: [-4.478820004113719,-4.478820004113719,-4.478820004113719,-4.478820004113719,-4.478820004113719,-4.478820004113719,-4.478820004113719,-4.478820004113719,-4.478820004113719,-4.478820004113719,-4.478820004113719,-4.478820004113719,-4.478820004113719,-4.478820004113719,-4.478820004113719,-4.478820004113719,-4.478820004113719,1.9909317407836316,2.4284259743294196,2.583607086568784,2.6221732085364384,2.5646239950735144,2.6508359909466,2.7523935377541604,2.60404649062666,2.571329929563901,2.498524200522748,2.5855941363275505,2.6203289885308654,2.541095176757595,2.5608151711178464,2.579648757812347,2.47671035793417,2.5509022058176325,2.48599677740884,2.474923714544546,2.499951526824691,2.4371082395435226,2.3774862305464315,2.317033993037322,2.340536693462234,2.3335741193199437,2.2613991395999284,2.191749027119452,2.135905149018242,2.189301964097548,2.154470077691398,2.1600058615885347,1.9211175743549873,1.9024656259421509,1.8807676407059282,1.924268425013445,1.7447991112059935,1.7459766261689234,1.5974855261228589,1.6096688336905227,1.5248742840352012,1.4941514871141846,1.5013369772994356,1.4260987396985363,1.3254940095797019,1.3476836639321297,1.2185101681756438,1.0451992303162778,1.10631628527296,0.9198701094105317,0.7026298827976409,0.7319113281226506,0.46748575659041874,0.288586012521293,0.23117181805826298,0.05747168527255868,0.07967733021544893,-0.08710801875456523,-0.33630682004393664,-0.4023776687802514,-0.47215977625117783,-0.7657514826514656,-1.2205733076580052,-1.3007839817529143,-0.9527663035539087,-0.8949587771658281,-2.0806337617166513,-2.1761574313106706,-2.17604152195516,-2.6869039789245734,-3.7856373747565546,-3.092400781670628,-2.6869587401833273,-3.3801438266621355,-0.6988439706992171]\n"
     ]
    }
   ],
   "source": [
    "# Print the coefficients and intercept for logistic regression\n",
    "print(\"Coefficients: \" + str(linearModel.coefficientMatrix))\n",
    "print(\"Intercept: \" + str(linearModel.interceptVector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+--------------------+\n",
      "|label|prediction|         probability|\n",
      "+-----+----------+--------------------+\n",
      "|   17|      20.0|[2.41103232471095...|\n",
      "|   17|      18.0|[2.37807091928234...|\n",
      "|   17|      18.0|[2.37795406494152...|\n",
      "|   17|      18.0|[2.38455178890594...|\n",
      "|   17|      19.0|[2.41614259994857...|\n",
      "|   17|      19.0|[2.41821986206058...|\n",
      "|   17|      19.0|[2.40694232223421...|\n",
      "|   17|      19.0|[2.40609218138815...|\n",
      "|   17|      20.0|[2.42326031107704...|\n",
      "|   17|      20.0|[2.42023086462885...|\n",
      "|   17|      19.0|[2.43277010000363...|\n",
      "|   17|      18.0|[2.38398015933459...|\n",
      "|   17|      18.0|[2.46521409456441...|\n",
      "|   17|      18.0|[2.37181770281334...|\n",
      "|   17|      23.0|[2.42398772111925...|\n",
      "|   17|      18.0|[2.38040155270063...|\n",
      "|   17|      18.0|[2.38299942877415...|\n",
      "|   17|      18.0|[2.39298827085825...|\n",
      "|   17|      18.0|[2.40294216352775...|\n",
      "|   17|      18.0|[2.31816811960394...|\n",
      "+-----+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on test data using the transform() method.\n",
    "predictions = linearModel.transform(test_data)\n",
    "selected = predictions.select(\"label\", \"prediction\", \"probability\")\n",
    "selected.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: 5.747%\n"
     ]
    }
   ],
   "source": [
    "def accuracy_m(model): \n",
    "    predictions = model.transform(test_data)\n",
    "    cm = predictions.select(\"label\", \"prediction\")\n",
    "    acc = cm.filter(cm.label == cm.prediction).count() / cm.count()\n",
    "    print(\"Model accuracy: %.3f%%\" % (acc * 100)) \n",
    "accuracy_m(model = linearModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "areaUnderROC\n"
     ]
    }
   ],
   "source": [
    "### Use ROC \n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Evaluate model\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "print(evaluator.evaluate(predictions))\n",
    "print(evaluator.getMetricName())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "# Create ParamGrid for Cross Validation\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(lr.regParam, [0.01, 0.5])\n",
    "             .build())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train model: 803.533 seconds\n"
     ]
    }
   ],
   "source": [
    "from time import *\n",
    "start_time = time()\n",
    "\n",
    "# Create 5-fold CrossValidator\n",
    "cv = CrossValidator(estimator=lr,\n",
    "                    estimatorParamMaps=paramGrid,\n",
    "                    evaluator=evaluator, numFolds=5)\n",
    "\n",
    "# Run cross validations\n",
    "cvModel = cv.fit(train_data)\n",
    "# likely take a fair amount of time\n",
    "end_time = time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(\"Time to train model: %.3f seconds\" % elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: 7.642%\n"
     ]
    }
   ],
   "source": [
    "accuracy_m(model = cvModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Param(parent='LogisticRegression_dba256522b9c', name='aggregationDepth', doc='suggested depth for treeAggregate (>= 2)'): 2,\n",
       " Param(parent='LogisticRegression_dba256522b9c', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty'): 0.0,\n",
       " Param(parent='LogisticRegression_dba256522b9c', name='family', doc='The name of family which is a description of the label distribution to be used in the model. Supported options: auto, binomial, multinomial.'): 'auto',\n",
       " Param(parent='LogisticRegression_dba256522b9c', name='featuresCol', doc='features column name'): 'features',\n",
       " Param(parent='LogisticRegression_dba256522b9c', name='fitIntercept', doc='whether to fit an intercept term'): True,\n",
       " Param(parent='LogisticRegression_dba256522b9c', name='labelCol', doc='label column name'): 'label',\n",
       " Param(parent='LogisticRegression_dba256522b9c', name='maxIter', doc='maximum number of iterations (>= 0)'): 10,\n",
       " Param(parent='LogisticRegression_dba256522b9c', name='predictionCol', doc='prediction column name'): 'prediction',\n",
       " Param(parent='LogisticRegression_dba256522b9c', name='probabilityCol', doc='Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities'): 'probability',\n",
       " Param(parent='LogisticRegression_dba256522b9c', name='rawPredictionCol', doc='raw prediction (a.k.a. confidence) column name'): 'rawPrediction',\n",
       " Param(parent='LogisticRegression_dba256522b9c', name='regParam', doc='regularization parameter (>= 0)'): 0.01,\n",
       " Param(parent='LogisticRegression_dba256522b9c', name='standardization', doc='whether to standardize the training features before fitting the model'): True,\n",
       " Param(parent='LogisticRegression_dba256522b9c', name='threshold', doc='threshold in binary classification prediction, in range [0, 1]'): 0.5,\n",
       " Param(parent='LogisticRegression_dba256522b9c', name='tol', doc='the convergence tolerance for iterative algorithms (>= 0)'): 1e-06}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bestModel = cvModel.bestModel\n",
    "bestModel.extractParamMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
